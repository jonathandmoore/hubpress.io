<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Jonathan Moore]]></title><description><![CDATA[Bio data fettling]]></description><link>https://jonathandmoore.github.io</link><generator>RSS for Node</generator><lastBuildDate>Fri, 16 Sep 2016 14:00:58 GMT</lastBuildDate><atom:link href="https://jonathandmoore.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Ensemble analysis of Differential Expression from RNA-Seq data sets]]></title><description><![CDATA[<div class="paragraph">
<p>Recently I&#8217;ve seen discussions among bioinformatians of which is 'the best' method for DE analysis of RNA-Seq data.  It&#8217;s a recurring theme, and mostly the answer is 'it depends on context and aims' although there is also evidence that some methods are just better than others.</p>
</div>
<div class="paragraph">
<p>For the past couple of years, my answer has been that the best method is 'all of them'.  I&#8217;ve come round to thinking that an ensemble of methods probably gives the best result most of the time.</p>
</div>
<div class="paragraph">
<p>This isn&#8217;t really a new problem or a new question.  Before RNA-Seq was <em>de rigeur</em> for expression experiments, we noticed that in microarray experiments, the list of DE genes varied depending on the method used to estimate DE.  In our projects at the time, the 'best' method for DE analysis was a source of regular debate in project meetings among bioinformaticians, or statisticians, or modellers.</p>
</div>
<div class="paragraph">
<p>We came to the conclusion that the most robust approach was to use multiple methods, and to correlate them.  For example, in this timeseries analysis <a href="http://www.plantcell.org/content/24/9/3530.full" class="bare">http://www.plantcell.org/content/24/9/3530.full</a>, we used two different methods for estimating whether genes were DE.  We then did a manual (or is that visual?) review of the expression profiles of the anomalous cases where only one of the methods score the gene as significantly DE.  This review led us to a conclusion about which method was more reliable in the edge cases for this particular data set.</p>
</div>
<div class="paragraph">
<p>Fast forward a bit and the same issue is live in RNA-Seq expression analysis.  You&#8217;ve aligned your reads or analysed your kmer content, and have a score or count for each gene or transcript, and you need to decide which tool to use to estimate DE.</p>
</div>
<div class="paragraph">
<p>This Venn diagram from Chen et al, 2015 was recently used to give a nice example of the problem:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="http://static-content.springer.com/image/art%3A10.1186%2F1471-2164-16-S7-S14/MediaObjects/12864_2015_Article_7208_Fig6_HTML.jpg" alt="12864 2015 Article 7208 Fig6 HTML.jpg">
</div>
</div>
<div class="paragraph">
<p>Each method comes up with a different list of genes that are DE. There is some overlap between methods, but it&#8217;s not immediately clear which method is 'right' or 'wrong' about which particular genes.</p>
</div>
<div class="paragraph">
<p>My answer is to believe all the methods, to some extent, and to take an ensemble of methods as my estimate of DE.</p>
</div>
<div class="paragraph">
<p><strong>The problem is how to take an ensemble of DE analysis methods.</strong></p>
</div>
<div class="paragraph">
<p>My insight, and my gut feeling, was that when we take the Venn diagram/p-value cut-off approach, then we throw away most of the information about DE.  The tyranny of p&lt;0.05 demands that genes are DE or not.  In reality, there is a whole spectrum from 'almost definitely not DE at all' to 'almost definitely DE'.  The analysis tools even make it easy for us by giving an output of q-values, or FDR values for each gene or transcript, that are essentially a metric of 'DE-ness'.</p>
</div>
<div class="paragraph">
<p>I decided to combine all these separate metrics into a single, ensemble, metric of DE-ness. A natural fettling approach suggested itself:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Use each tool to generate a ranked order list of all genes,
       sorted by q-value or FDR.
For each gene, calculate its sum of ranks,
       by adding up the ranks each individual method had given the gene.
Sort the gene list by this rank-sum metric.</pre>
</div>
</div>
<div class="paragraph">
<p>This gave me a list of genes, sorted from most likely to be DE, as estimated by all the methods, to least likely.  Bingo, I have an ensemble DE metric.</p>
</div>
<div class="paragraph">
<p>The only thing that remains is to answer the biologist&#8217;s key question "Which genes are DE?".  How to choose where to cut-off the list?</p>
</div>
<div class="paragraph">
<p>My view on cut-offs is to cut the list to suit my needs.</p>
</div>
<div class="paragraph">
<p>If you want a strict cut-off, because you are going to spend money on follow-up qPCR experiments, then take a cutoff where all genes above the cut-off are assessed as significant by all methods.  If you want a lax cut-off, because you want sensitivity, and you are going to follow up with a cheap bioinformatic method like functional enrichment testing, then choose the cutoff accordingly - cut-off at the point where all genes above the cutoff were found significant by at least one method, or at least two.</p>
</div>
<div class="paragraph">
<p>I think there may also be principled ways to come up with a cut-off.</p>
</div>
<div class="paragraph">
<p>One would expect that there is a region at the significant end of the list, where the rank correlation between methods is high, and a region at the non-significant end of the list where rank correlation between methods is low to non-existent.  One could try to detect the zone where the rank correlation starts to decay to an unacceptable degree.</p>
</div>
<div class="paragraph">
<p>A different method of choosing a principled cutoff might be through some kind of leave-one-out cross-validation approach.</p>
</div>
<div class="paragraph">
<p>This ensemble approach is flexible and extensible to as many different methods of DE assessment as you can choose.  My initial choices might be DESeq2, EdgeR, BaySeq, cuffdiff, and limma-voom, because they all have such different models of what it means to be DE, so are likely to form an interesting ensemble.  You might prefer your own or different methods.</p>
</div>
<div class="paragraph">
<p>I think there is a right answer to "Which methods should I use in my ensemble?" and I think it is "All of them".</p>
</div>
<div class="paragraph">
<p>The point is, all methods tend to have a sweet spot where they are particularly sensitive and/or specific, so the more methods you use in your ensemble, the more potential you have for a generally good ensemble.</p>
</div>
<div class="paragraph">
<p>In our first public outing for this method (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3915549/" class="bare">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3915549/</a>) we used three different methods of DE assessment, and we cutoff at the point where at least two methods classified a gene as significant.  Our approach was queried by one of our anonymous reviewers.  They asked whether this was an accepted method, and whether we could point to evidence that it would work.  We responded with logic, rather than direct evidence.  We showed that we were using a number of well-cited accepted methods, and correlating their results for robustness.  We also showed that the burgeoning fields of Data Science, and competitive mathematical modelling, were rich with examples of ensemble methods outperforming a single method.  Our argument was accepted.</p>
</div>
<div class="paragraph">
<p>The obvious extension of the approach is to start weighting the methods in the ensemble, either based on prior information about how good they are, or where their sweet spots are, or using an iterative approach based on the data in hand.  I might get back to that later.</p>
</div>
<div class="paragraph">
<p>Have you used an ensemble method for estimating differential gene expression?  Do you think this is a worthwhile approach?</p>
</div>]]></description><link>https://jonathandmoore.github.io/2016/09/12/Ensemble-analysis-of-Differential-Expression-from-RNA-Seq-data-sets.html</link><guid isPermaLink="true">https://jonathandmoore.github.io/2016/09/12/Ensemble-analysis-of-Differential-Expression-from-RNA-Seq-data-sets.html</guid><category><![CDATA[RNA-Seq]]></category><category><![CDATA[ Ensemble]]></category><category><![CDATA[ Differential Expression]]></category><dc:creator><![CDATA[Jonathan Moore]]></dc:creator><pubDate>Mon, 12 Sep 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Rhizosphere metatranscriptome analysis  0/n]]></title><description><![CDATA[<div class="paragraph">
<p>I&#8217;m a co-investigator on the BBSRC/NERC-funded Roots of Decline project looking at the interactions between crop rotations, rhizosphere microbiome and oilseed rape crop yield.  The project has been tweeting progress <a href="https://twitter.com/BBSRC_SARISA" class="bare">https://twitter.com/BBSRC_SARISA</a> and it&#8217;s been interesting to see the combination of experimental plots, field sampling from farms, and lab work.</p>
</div>
<div class="paragraph">
<p>We&#8217;ve done some 16S, 18S and ITS analysis on the field samples, and now the first metatranscriptome samples are in library prep for Illumina sequencing, so it&#8217;s time for some fettling.</p>
</div>
<div class="paragraph">
<p>I took a look at the EBI metagenomics portal, and took some inspiration from their pipeline: <a href="https://www.ebi.ac.uk/metagenomics/about" class="bare">https://www.ebi.ac.uk/metagenomics/about</a></p>
</div>
<div class="paragraph">
<p>My initial plan for our metatranscriptome analysis goes like this:</p>
</div>
<div class="paragraph">
<p>First the usual <a href="http://earlham.ac.uk" class="bare">http://earlham.ac.uk</a> primary analysis on the data, comprising:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Demultiplexing with bcl2fastq
FASTQC
Kontaminant screening of a random sample of reads, for likely contaminants
MEGAN screening of a random sample of reads, for a taxonomic overview of the sample</pre>
</div>
</div>
<div class="paragraph">
<p>Providing the quality looks good, we&#8217;ll move on to develop the metatranscriptome analysis.  First a clean-up.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Trim using Trimmomatic to remove low quality segments and adaptors
Assemble read pairs using PANDAseq (probably)</pre>
</div>
</div>
<div class="paragraph">
<p>Then comes the first fork in the road. We would like to treat ncRNA and mRNA sequence reads differently in the subsequent steps in the analysis.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Use SortMeRNA to classify sequence reads containing rRNA
   Something to pull out tRNAs</pre>
</div>
</div>
<div class="paragraph">
<p>There are some complexities with this, as the SortMeRNA rRNA database may not be very sensitive for soil organisms, particularly eukaryotes. There is a lot of underexplored genomic dark matter in the soil, so I&#8217;m told, so we may do some experimenting with various alternative rRNA databases.</p>
</div>
<div class="paragraph">
<p>Once we have the ncRNA/mRNA classification of each molecule nailed down, on to the detailed classifications.</p>
</div>
<div class="paragraph">
<p>Using rRNA sequences to do taxonomic classification is relatively well explored territory.  We will most likely stick to a Qiime analysis here, but again, we are likely to want to use a specialised database for classification rather than SILVA or GreenGenes.</p>
</div>
<div class="paragraph">
<p>Classifying mRNA sequences is likely a bit more tricky.</p>
</div>
<div class="paragraph">
<p>I recently read that if you ask three different classification methods to classify a sequence read, you are only likely to get two of them to agree on a genus classification about 60% of the time, and only then on reads of 500nt.  Species-level classification is even worse.  Functional classification is even worse still <a href="http://www.nature.com/articles/srep19233" class="bare">http://www.nature.com/articles/srep19233</a>.</p>
</div>
<div class="paragraph">
<p>WEVote looks promising in this respect, and I think it&#8217;s a rational approach to taxonomic classification, but I haven&#8217;t had a chance to trying implementing it yet: <a href="http://biorxiv.org/content/biorxiv/early/2016/05/22/054205.full.pdf" class="bare">http://biorxiv.org/content/biorxiv/early/2016/05/22/054205.full.pdf</a></p>
</div>
<div class="paragraph">
<p>With that in mind, we will be using a couple of different methods for both taxonomic and functional classification, and correlating the results.  We will also be paying careful attention to the databases we use for the classifications, as this has as much effect, if not more, than the chosen algorithm.</p>
</div>
<div class="paragraph">
<p>More later.</p>
</div>]]></description><link>https://jonathandmoore.github.io/2016/09/08/Rhizosphere-metatranscriptome-analysis-0n.html</link><guid isPermaLink="true">https://jonathandmoore.github.io/2016/09/08/Rhizosphere-metatranscriptome-analysis-0n.html</guid><dc:creator><![CDATA[Jonathan Moore]]></dc:creator><pubDate>Thu, 08 Sep 2016 00:00:00 GMT</pubDate></item></channel></rss>